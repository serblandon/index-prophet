{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPnQVEHWUi/anQGcFIsicZe"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":18,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"--f-QozzMVdD","executionInfo":{"status":"ok","timestamp":1713720416511,"user_tz":-180,"elapsed":810,"user":{"displayName":"Serban Crasovan","userId":"13133037493290624473"}},"outputId":"faaf9ba4-c8b7-4529-9161-f0dae0ae2a24"},"outputs":[{"output_type":"stream","name":"stdout","text":["503\n"]}],"source":["# Fetch List of sp500 tickers\n","import pandas as pd\n","import pandas_datareader as pdr\n","from datetime import datetime\n","\n","def get_sp500_tickers():\n","  table = pd.read_html('https://en.wikipedia.org/wiki/List_of_S%26P_500_companies')\n","  sp500_df = table[0]\n","\n","  tickers = sp500_df['Symbol'].tolist()\n","\n","  tickers = [ticker.replace('.', '-') for ticker in tickers]\n","\n","  return tickers\n","\n","\n","sp500_tickers = get_sp500_tickers()\n","print(len(sp500_tickers))"]},{"cell_type":"code","source":["import yfinance as yf\n","import zipfile\n","import os\n","from IPython.display import HTML\n","\n","# Fetch csv files with historical data for each ticker containing only Date and Close columns\n","\n","zip_filename = 'historical_sp500_data.zip'\n","start_date = '2007-01-01'\n","end_date ='2024-04-19'\n","\n","# Setup the output directory\n","output_directory = './stock_data'\n","os.makedirs(output_directory, exist_ok=True)\n","\n","with zipfile.ZipFile(zip_filename, 'w') as zipf:\n","  for ticker in sp500_tickers:\n","    # stock_data = yf.download(ticker, start=start_date, end=end_date)\n","\n","    # close_data = stock_data[['Close']].reset_index()\n","\n","    filename = os.path.join(output_directory, f\"{ticker}.csv\")\n","    # close_data.to_csv(filename)\n","\n","    # Add the file to the zip\n","    zipf.write(filename, arcname=os.path.basename(filename))\n","\n","\n"],"metadata":{"id":"eU2VXHnnM_ju","executionInfo":{"status":"ok","timestamp":1713717045147,"user_tz":-180,"elapsed":804,"user":{"displayName":"Serban Crasovan","userId":"13133037493290624473"}}},"execution_count":12,"outputs":[]},{"cell_type":"code","source":["from google.colab import files\n","\n","files.download(zip_filename)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":17},"id":"X-5hHaalY5wa","executionInfo":{"status":"ok","timestamp":1713717049067,"user_tz":-180,"elapsed":314,"user":{"displayName":"Serban Crasovan","userId":"13133037493290624473"}},"outputId":"3579fa58-c2b1-4d21-89af-a6ddaa0b9b8d"},"execution_count":13,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["\n","    async function download(id, filename, size) {\n","      if (!google.colab.kernel.accessAllowed) {\n","        return;\n","      }\n","      const div = document.createElement('div');\n","      const label = document.createElement('label');\n","      label.textContent = `Downloading \"${filename}\": `;\n","      div.appendChild(label);\n","      const progress = document.createElement('progress');\n","      progress.max = size;\n","      div.appendChild(progress);\n","      document.body.appendChild(div);\n","\n","      const buffers = [];\n","      let downloaded = 0;\n","\n","      const channel = await google.colab.kernel.comms.open(id);\n","      // Send a message to notify the kernel that we're ready.\n","      channel.send({})\n","\n","      for await (const message of channel.messages) {\n","        // Send a message to notify the kernel that we're ready.\n","        channel.send({})\n","        if (message.buffers) {\n","          for (const buffer of message.buffers) {\n","            buffers.push(buffer);\n","            downloaded += buffer.byteLength;\n","            progress.value = downloaded;\n","          }\n","        }\n","      }\n","      const blob = new Blob(buffers, {type: 'application/binary'});\n","      const a = document.createElement('a');\n","      a.href = window.URL.createObjectURL(blob);\n","      a.download = filename;\n","      div.appendChild(a);\n","      a.click();\n","      div.remove();\n","    }\n","  "]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["download(\"download_50f74c91-9393-485e-8e90-8804ea0013fe\", \"historical_sp500_data.zip\", 67222014)"]},"metadata":{}}]},{"cell_type":"code","source":["import pandas as pd\n","import os\n","\n","# Directory where the CSV files are stored\n","output_directory = './stock_data'\n","\n","# List all files in the directory\n","files = os.listdir(output_directory)\n","\n","# Loop through each file in the directory\n","for file in files:\n","    if file.endswith('.csv'):\n","        # Construct the full file path\n","        file_path = os.path.join(output_directory, file)\n","\n","        # Read the CSV file into a DataFrame\n","        df = pd.read_csv(file_path)\n","\n","        # Extract ticker from filename (remove the .csv extension)\n","        ticker = file.replace('.csv', '')\n","\n","        # Add the new column 'ticker' with the filename as its value\n","        df.insert(0, 'ticker', ticker)\n","\n","        # Save the modified DataFrame back to the CSV\n","        df.to_csv(file_path, index=False)\n","        print(f\"Updated file: {file_path}\")\n"],"metadata":{"id":"U427qU1acvvK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Check if 'Unnamed: 0' is in the DataFrame and drop it if present\n","\n","for file in files:\n","  file_path = os.path.join(output_directory, file)\n","        # Read the CSV file into a DataFrame\n","  df = pd.read_csv(file_path)\n","  if 'Unnamed: 0' in df.columns:\n","      df.drop(columns=['Unnamed: 0'], inplace=True)\n","  df.to_csv(file_path, index=False)\n"],"metadata":{"id":"v0ipFG3zdzbn","executionInfo":{"status":"ok","timestamp":1713716626963,"user_tz":-180,"elapsed":11168,"user":{"displayName":"Serban Crasovan","userId":"13133037493290624473"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","source":["import pandas as pd\n","import os\n","\n","# Directory containing your CSV files\n","input_directory = './stock_data'\n","output_file = './merged_historical_sp500.csv'\n","\n","# List to hold data from each CSV file\n","dataframes = []\n","\n","# Loop through all CSV files in the directory\n","for filename in os.listdir(input_directory):\n","    if filename.endswith('.csv'):\n","        filepath = os.path.join(input_directory, filename)\n","        # Read the CSV file and add it to the list\n","        df = pd.read_csv(filepath)\n","        # Optionally, verify or set the correct column names if they differ\n","        # df.columns = ['ticker', 'date', 'adj_close_price']\n","        dataframes.append(df)\n","\n","# Concatenate all dataframes into a single dataframe\n","merged_df = pd.concat(dataframes, ignore_index=True)\n","\n","# Save the merged dataframe to a new CSV file\n","merged_df.to_csv(output_file, index=False)\n","\n","print(f\"Merged CSV saved to {output_file}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Qlp15ZrHnIgL","executionInfo":{"status":"ok","timestamp":1713719015293,"user_tz":-180,"elapsed":9674,"user":{"displayName":"Serban Crasovan","userId":"13133037493290624473"}},"outputId":"a328655f-e878-4405-dc46-6ca79d95ac29"},"execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":["Merged CSV saved to ./merged_historical_sp500.csv\n"]}]}]}